"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[423],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>f});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=d(n),m=r,f=p["".concat(s,".").concat(m)]||p[m]||c[m]||l;return n?a.createElement(f,o(o({ref:t},u),{},{components:n})):a.createElement(f,o({ref:t},u))}));function f(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,o=new Array(l);o[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[p]="string"==typeof e?e:r,o[1]=i;for(var d=2;d<l;d++)o[d]=n[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},55007:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>l,metadata:()=>i,toc:()=>d});var a=n(87462),r=(n(67294),n(3905));const l={sidebar_position:2},o="Getting Started",i={unversionedId:"embedds/getting-started",id:"embedds/getting-started",title:"Getting Started",description:"Docker Images",source:"@site/docs/embedds/getting-started.md",sourceDirName:"embedds",slug:"/embedds/getting-started",permalink:"/anansi/embedds/getting-started",draft:!1,editUrl:"https://github.com/infrawhispers/anansi/tree/main/docs/docs/embedds/getting-started.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Quick Evaluation",permalink:"/anansi/embedds/quick-evaluation"},next:{title:"Supported Models",permalink:"/anansi/embedds/supported-models"}},s={},d=[{value:"Docker Images",id:"docker-images",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:3},{value:"Config Files",id:"config-files",level:3},{value:"Issuing Requests",id:"issuing-requests",level:2}],u={toc:d},p="wrapper";function c(e){let{components:t,...n}=e;return(0,r.kt)(p,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"getting-started"},"Getting Started"),(0,r.kt)("h2",{id:"docker-images"},"Docker Images"),(0,r.kt)("p",null,"The easiest way to get started locally is to use one of the docker images we publish on DockerHub:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://hub.docker.com/repository/docker/infrawhispers/anansi/tags?page=1&ordering=last_updated&name=embeddings-latest"},"latest")," - includes CUDA and libcudnn bindings to support GPU and CPU accelerated inference."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://hub.docker.com/repository/docker/infrawhispers/anansi/tags?page=1&ordering=last_updated&name=embeddings-latest-cpu"},"latest-cpu")," - a minimial image, lacking CUDA + libcuddn dylibs that allows for ",(0,r.kt)("strong",{parentName:"li"},"only")," CPU inference.")),(0,r.kt)("p",null,"Once you have a docker image, you can then simply run it using ",(0,r.kt)("inlineCode",{parentName:"p"},"docker-compose")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"docker"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"docker run -p 0.0.0.0:50051:50051 \\\n           -p 0.0.0.0:50052:50052 \\\n           -v $(PWD)/.cache:/app/.cache \\\n           anansi:embeddings-latest-cpu\n")),(0,r.kt)("p",null,"With the above setup, you can then send HTTP1 requests to 50051 or issue gRPC requests to 50052."),(0,r.kt)("h2",{id:"configuration"},"Configuration"),(0,r.kt)("h3",{id:"environment-variables"},"Environment Variables"),(0,r.kt)("p",null,"embedds can be configured using environment variables. All environment variables are prefixed with ",(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_")," and are outlined below, along with their effects:"),(0,r.kt)("p",null,"The list of environment variables that are supported are as follows:"),(0,r.kt)("table",null,(0,r.kt)("tr",null,(0,r.kt)("td",null,(0,r.kt)("b",null,"Environment Variable")),(0,r.kt)("td",null,(0,r.kt)("b",null,"Usage"))),(0,r.kt)("tr",null,(0,r.kt)("td",null,(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_GRPC_PORT"))),(0,r.kt)("td",null,(0,r.kt)("p",null,"port to listen for and server gRPC requests [default=50051]"))),(0,r.kt)("tr",null,(0,r.kt)("td",null,(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_HTTP_PORT"))),(0,r.kt)("td",null,(0,r.kt)("p",null,"port to listen for and server HTTP requests [default=50052]"))),(0,r.kt)("tr",null,(0,r.kt)("td",null,(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_CONFIG_FILE"))),(0,r.kt)("td",null,(0,r.kt)("p",null,"filepath to store the runtime configuration for models - more on this file is available below [default=/app/config.yaml] "))),(0,r.kt)("tr",null,(0,r.kt)("td",null,(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_CACHE_FOLDER"))),(0,r.kt)("td",null,(0,r.kt)("p",null,"folder in which to store the cached model files - these are typically on the order of ~100s of MBs and can grow to GBs if you bin-pack different types of models [default=/app/.cache] "))),(0,r.kt)("tr",null,(0,r.kt)("td",null,(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_ALLOW_ADMIN"))),(0,r.kt)("td",null,(0,r.kt)("p",null,(0,r.kt)("p",null,"whether or not to honor ",(0,r.kt)("inlineCode",{parentName:"p"},"Initalize(..)")," requests, which load a model into the current ONNX runtime. we recommend that models be loaded once on startup; however, this conveninece is included for experimentation ","[default=false]"))))),(0,r.kt)("h3",{id:"config-files"},"Config Files"),(0,r.kt)("p",null,"The `EMBEDDS_CONFIG_FILE` points to an accessible filepath that stores a list of models that should be instantiated on startup of the process.",(0,r.kt)("b",null,"If these models are missing, embedds will attempt to download them and store them"),(0,r.kt)("p",null,"in the filepath pointed to by ",(0,r.kt)("inlineCode",{parentName:"p"},"EMBEDDS_CACHE_FOLDER"),". An example configuration is outlined below:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"models:\n  # class must match one of the available models, defined at:\n  # https://github.com/infrawhispers/anansi/blob/main/embeddings/proto/api.proto\n  - name: VIT_L_14_336_OPENAI\n    class: ModelClass_CLIP\n    # [optional] set to zero or leave empty for parallelism to be determined\n    num_threads: 4\n    # [optional] enable | disable parallel execution of the onnx graph, which may improve\n    # performance at the cost of memory usage.\n    parallel_execution: true\n  - name: INSTRUCTOR_LARGE\n    class: ModelClass_INSTRUCTOR\n  - name: INSTRUCTOR_LARGE\n    class: ModelClass_INSTRUCTOR\n")),(0,r.kt)("p",null,"This configuration would create ONE ",(0,r.kt)("inlineCode",{parentName:"p"},"VIT_L_14_336_OPENAI")," and TWO ",(0,r.kt)("inlineCode",{parentName:"p"},"INSTRUCTOR_LARGE")," models. This is useful for running multiple embedding models on a single GPU. The list of devices and available models can be found ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/infrawhispers/anansi/blob/main/embeddings/proto/api.proto"},"here"),". By default, embedds will instantiate one instance of ",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/hkunlp/instructor-large"},"INSTRUCTOR_LARGE"),"."),(0,r.kt)("h2",{id:"issuing-requests"},"Issuing Requests"),(0,r.kt)("p",null,"With a running server, you can now issue requests against the server. Take a look at the ",(0,r.kt)("a",{href:"/swagger-api/embedds",target:"_blank"},"swagger-api")," docs for the HTTP methods available to you."),(0,r.kt)("br",null),"You can also pull the ",(0,r.kt)("a",{href:"https://github.com/infrawhispers/anansi/blob/main/embedds/proto/api.proto",target:"_blank"},"proto definition")," from source to build your grpc client. Native clients are on the roadmap, we are also open for pull-requests \ud83e\udd29.")}c.isMDXComponent=!0}}]);